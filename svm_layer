import numpy as np
import tensorflow as tf

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# CONFIGURATION
tf.reset_default_graph()
lr = 1E-3
n_iterations, batch_size, print_every = 2**14 , 2**8, 2**9
n_classes = 10
C = 0.4
dtype, shape = tf.float32, [None, 28, 28]

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# DATA
(train, label_train), (test, label_test) = tf.keras.datasets.mnist.load_data()

num_test, num_train = 10000, 60000
test, label_test = test[:num_test], label_test[:num_test]
train, label_train = train[:num_train], label_train[:num_train]

test = test.astype(np.float32)/255
train = train.astype(np.float32)/255

    

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# NETWORK
def network(X):
  x = tf.layers.flatten(X)
  dense1 = tf.layers.dense(x, 800, tf.nn.sigmoid)
  dense2 = tf.layers.dense(dense1, 200, tf.nn.sigmoid)
  y = tf.layers.dense(dense2, 12)
  return y

def soft(X):
  return tf.nn.softmax(X)

def svm(X):
  with tf.variable_scope("svm", reuse=tf.AUTO_REUSE):
    w = tf.get_variable("svm_w", [n_classes, X.shape[1]])
  y = tf.tensordot(X, w, axes=[[1], [1]])
  return y

def softmax_model(X):
  return soft(network(X))

def svm_model(X):
  return svm(network(X))
  

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# TRAINING & TESTING
def loss_f_soft(Y, y):
  return tf.reduce_mean(-tf.reduce_sum(Y * tf.log(y + 1E-10), reduction_indices=[1]))

def loss_f_svm(Y, y):
  with tf.variable_scope("svm", reuse=tf.AUTO_REUSE):
    w = tf.get_variable("svm_w")
  return tf.reduce_sum(0.5 * tf.reduce_sum( tf.multiply( w, w ), 1 ) + \
         C * tf.reduce_sum(tf.maximum(1 - y * Y, 0), axis=0))

def optimizer(loss):
  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
  opt = tf.train.AdamOptimizer(lr).minimize(loss)
  return tf.group([opt, update_ops])

def train_test():
  with tf.Session() as sess:
      sess.run(init)
      for iteration in range(n_iterations):
          indices = np.random.choice(len(train), batch_size)
          batch_X, batch_Y = train[indices], label_train[indices]
          feed_dict = {X: batch_X, Y_n: batch_Y}
          _, lo = sess.run([step, loss], feed_dict=feed_dict)
          
          if iteration % print_every == 0:
            print("loss", lo)
            acc = sess.run(accuracy, feed_dict={X: test, Y_n: label_test})
            print("acc ", acc)
            if acc > .98:
              break
              
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++              
# GOALS
tf.reset_default_graph()
X = tf.placeholder(dtype, shape)
Y_n = tf.placeholder(tf.int32, [None])
Y = tf.one_hot(Y_n, n_classes, off_value=-1., dtype=dtype) 
y = svm_model(X)
loss = loss_f_svm(Y,y)
step = optimizer(loss)
hits = tf.equal(tf.argmax(y,1), tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(hits, tf.float32))
init = tf.global_variables_initializer()

# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# MAIN
train_test()

